{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßÆ Roll Rate Model (Markov Chain)\n",
    "### Demo Notebook - Offline Mode (Parquet, No Oracle)\n",
    "This notebook loads a parquet dataset, builds transition matrices, runs 12‚Äëmonth forecast and performs backtesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Project root added: D:\\Python_code\\RR_Model\n",
      "üîç src module found: ModuleSpec(name='src', loader=<_frozen_importlib_external.SourceFileLoader object at 0x000001CFE68A3880>, origin='D:\\\\Python_code\\\\RR_Model\\\\src\\\\__init__.py', submodule_search_locations=['D:\\\\Python_code\\\\RR_Model\\\\src'])\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ‚úÖ FIX PATH FOR NOTEBOOK INSIDE /notebooks\n",
    "# ============================================================\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "# X√°c ƒë·ªãnh th∆∞ m·ª•c cha c·ªßa notebooks/\n",
    "project_root = Path(os.getcwd()).parent  # => C:\\Users\\User\\RR_model\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))  # ‚ö†Ô∏è th√™m project_root, KH√îNG th√™m src\n",
    "\n",
    "print(\"üìÅ Project root added:\", project_root)\n",
    "\n",
    "# Ki·ªÉm tra l·∫°i\n",
    "import importlib.util\n",
    "print(\"üîç src module found:\", importlib.util.find_spec(\"src\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Import th√†nh c√¥ng.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Sau ƒë√≥ import nh∆∞ b√¨nh th∆∞·ªùng:\n",
    "from src.config import CFG, BUCKETS_CANON, OUT_ROOT, PARQUET_DIR, PARQUET_FILE\n",
    "from src.rollrate.transition import compute_transition\n",
    "from src.rollrate.forecast import forecast_report\n",
    "from src.rollrate.transition import compute_transition_by_mob\n",
    "from collections import defaultdict\n",
    "from src.rollrate.backtest import (\n",
    "    matrix_stability_score,\n",
    "    rollforward_validation,\n",
    "    plot_matrix_diff,\n",
    "    plot_distribution_compare,\n",
    ")\n",
    "from src.data_loader import load_data\n",
    "\n",
    "print(\"‚úÖ Import th√†nh c√¥ng.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Loading data from Oracle...\n",
      "=== SQL DEBUG ===\n",
      "File: D:\\Python_code\\RR_Model\\sql\\NTB.sql\n",
      "First 200 chars:\n",
      " select  CUTOFF_DATE, AGREEMENT_ID, DISBURSAL_DATE, DISBURSAL_AMOUNT, DPD_EOM, RISK_BUCKET, PRINCIPLE_OUTSTANDING, STATUS, MOB, MAFC_SUB_CATEGORY PRODUCT_TYPE, NPA_STAGEID, DPD_GROUP, STATE_MODEL, CASE...\n",
      "Params: {}\n",
      "=================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python_code\\RR_Model\\src\\db.py:63: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(final_sql, conn, params=bind_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Added CUTOFF_DATE from DISBURSAL_DATE + MOB months\n",
      "  DISBURSAL_DATE  MOB CUTOFF_DATE\n",
      "0     2023-01-31    0  2023-01-31\n",
      "1     2023-01-31    1  2023-02-28\n",
      "2     2023-01-31    2  2023-03-31\n",
      "3     2023-01-31    3  2023-04-30\n",
      "4     2023-01-31    4  2023-05-31\n",
      "5     2023-01-31    5  2023-06-30\n",
      "6     2023-01-31    6  2023-07-31\n",
      "7     2023-01-31    7  2023-08-31\n",
      "8     2023-01-31    8  2023-09-30\n",
      "9     2023-01-31    9  2023-10-31\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUTOFF_DATE</th>\n",
       "      <th>AGREEMENT_ID</th>\n",
       "      <th>DISBURSAL_DATE</th>\n",
       "      <th>DISBURSAL_AMOUNT</th>\n",
       "      <th>DPD_EOM</th>\n",
       "      <th>RISK_BUCKET</th>\n",
       "      <th>PRINCIPLE_OUTSTANDING</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>MOB</th>\n",
       "      <th>PRODUCT_TYPE</th>\n",
       "      <th>NPA_STAGEID</th>\n",
       "      <th>DPD_GROUP</th>\n",
       "      <th>STATE_MODEL</th>\n",
       "      <th>RISK_SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>4089415</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>18309000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>B0</td>\n",
       "      <td>18309000</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>SALPIL</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>DPD0</td>\n",
       "      <td>DPD0</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-28</td>\n",
       "      <td>4089415</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>18309000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>B0</td>\n",
       "      <td>18273533</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>SALPIL</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>DPD0</td>\n",
       "      <td>DPD0</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>4089415</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>18309000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>B0</td>\n",
       "      <td>17959922</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>SALPIL</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>DPD0</td>\n",
       "      <td>DPD0</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-04-30</td>\n",
       "      <td>4089415</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>18309000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>B0</td>\n",
       "      <td>17703148</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>SALPIL</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>DPD0</td>\n",
       "      <td>DPD0</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>4089415</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>18309000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>B0</td>\n",
       "      <td>17414031</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>SALPIL</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>DPD0</td>\n",
       "      <td>DPD0</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CUTOFF_DATE  AGREEMENT_ID DISBURSAL_DATE  DISBURSAL_AMOUNT  DPD_EOM  \\\n",
       "0  2023-01-31       4089415     2023-01-31        18309000.0        0   \n",
       "1  2023-02-28       4089415     2023-01-31        18309000.0        0   \n",
       "2  2023-03-31       4089415     2023-01-31        18309000.0        0   \n",
       "3  2023-04-30       4089415     2023-01-31        18309000.0        0   \n",
       "4  2023-05-31       4089415     2023-01-31        18309000.0        0   \n",
       "\n",
       "  RISK_BUCKET  PRINCIPLE_OUTSTANDING STATUS  MOB PRODUCT_TYPE NPA_STAGEID  \\\n",
       "0          B0               18309000      A    0       SALPIL     REGULAR   \n",
       "1          B0               18273533      A    1       SALPIL     REGULAR   \n",
       "2          B0               17959922      A    2       SALPIL     REGULAR   \n",
       "3          B0               17703148      A    3       SALPIL     REGULAR   \n",
       "4          B0               17414031      A    4       SALPIL     REGULAR   \n",
       "\n",
       "  DPD_GROUP STATE_MODEL RISK_SCORE  \n",
       "0      DPD0        DPD0          B  \n",
       "1      DPD0        DPD0          B  \n",
       "2      DPD0        DPD0          B  \n",
       "3      DPD0        DPD0          B  \n",
       "4      DPD0        DPD0          B  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load parquet dataset\n",
    "df = load_data(r\"D:\\Python_code\\RR_Model\\sql\\NTB.sql\")\n",
    "import pandas as pd\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "\n",
    "# ƒê·∫£m b·∫£o DISBURSAL_DATE ƒë√∫ng ƒë·ªãnh d·∫°ng datetime\n",
    "df[\"DISBURSAL_DATE\"] = pd.to_datetime(df[\"DISBURSAL_DATE\"], format=\"%m/%d/%Y %H:%M\", errors=\"coerce\")\n",
    "df[\"DISBURSAL_DATE\"] = df[\"DISBURSAL_DATE\"] + MonthEnd(0)\n",
    "df[\"MOB\"] = df[\"MOB\"].fillna(0).astype(int)\n",
    "\n",
    "# ‚úÖ T√≠nh cutoff cho t·ª´ng d√≤ng\n",
    "df[\"CUTOFF_DATE\"] = df.apply(lambda x: x[\"DISBURSAL_DATE\"] + MonthEnd(x[\"MOB\"]), axis=1)\n",
    "\n",
    "print(\"‚úÖ Added CUTOFF_DATE from DISBURSAL_DATE + MOB months\")\n",
    "print(df[[\"DISBURSAL_DATE\", \"MOB\", \"CUTOFF_DATE\"]].head(10))\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute simple per-product transitions from the whole dataset\n",
    "matrices = {}\n",
    "for subprod in df['PRODUCT_TYPE'].dropna().unique():\n",
    "    sub_df = df[df['PRODUCT_TYPE'] == subprod].copy()\n",
    "    sub_df['COUNT'] = 1\n",
    "    matrices[subprod] = compute_transition(sub_df, value_col='COUNT')\n",
    "list(matrices.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rollrate.transition import compute_transition_by_mob\n",
    "\n",
    "matrices_by_mob = compute_transition_by_mob(df)\n",
    "\n",
    "for seg, mob_dict in matrices_by_mob.items():\n",
    "    print(f\"\\nSegment {seg}: {len(mob_dict)} MOB matrices\")\n",
    "    print(\"MOBs:\", list(mob_dict.keys())[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast 12 months using the latest snapshot\n",
    "latest_month = df[CFG['cutoff']].max()\n",
    "df_latest = df[df[CFG['cutoff']] == latest_month].copy()\n",
    "reports, summary = forecast_report(df_latest, matrices, months=12, value_col=CFG['ead'])\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest: build a noisy matrix and compare\n",
    "P = list(matrices.values())[0]\n",
    "P2 = (P * (1 + np.random.normal(0, 0.05, P.shape))).clip(lower=0)\n",
    "P2 = P2.div(P2.sum(axis=1), axis=0)\n",
    "score = matrix_stability_score(P, P2)\n",
    "print(f'Matrix Stability Score: {score:.4f}')\n",
    "plot_matrix_diff(P, P2, title='Œî Transition Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sau khi ƒë√£ c√≥ df[\"CUTOFF_DATE\"]\n",
    "df[\"CUTOFF_DATE_STR\"] = df[\"CUTOFF_DATE\"].dt.strftime(\"%Y%m\")\n",
    "\n",
    "# R·ªìi c·∫≠p nh·∫≠t CFG t·∫°m th·ªùi\n",
    "CFG[\"cutoff\"] = \"CUTOFF_DATE_STR\"\n",
    "\n",
    "# Gi·ªù g·ªçi l·∫°i backtest\n",
    "start_month = str(sorted(df[CFG['cutoff']].unique())[0])\n",
    "rf = rollforward_validation(df, P, start_month=start_month, horizon=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roll-forward validation from the first available month\n",
    "start_month = str(sorted(df[CFG['cutoff']].unique())[0])\n",
    "rf = rollforward_validation(df, P, start_month=start_month, horizon=2)\n",
    "plot_distribution_compare(rf, title='Roll-forward (Predicted vs Actual)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "summary.to_excel(OUT_ROOT / 'forecast_summary_demo.xlsx', index=False)\n",
    "print(f'‚úÖ Demo completed | Output saved at: {OUT_ROOT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"S·ªë b·∫£ng transition hi·ªán c√≥: {len(matrices_by_mob)}\")\n",
    "print(\"Danh s√°ch c√°c segment/product:\")\n",
    "for name in matrices_by_mob.keys():\n",
    "    print(\" -\", name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rollrate.transition import compute_transition_by_mob\n",
    "\n",
    "matrices_by_mob = compute_transition_by_mob(df)\n",
    "\n",
    "for seg, mob_dict in matrices_by_mob.items():\n",
    "    print(f\"\\nSegment {seg}: {len(mob_dict)} MOB matrices\")\n",
    "    print(\"MOBs:\", list(mob_dict.keys())[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seg, mob_dict in matrices_by_mob.items():\n",
    "    print(f\"\\nSegment {seg}: {len(mob_dict)} MOB matrices\")\n",
    "    print(\"MOB list:\", list(mob_dict.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = matrices_by_mob[\"A\"][1]\n",
    "print(mat.round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Row sums:\", mat.sum(axis=1).round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1) C·ªôt state & ead c√≥ t·ªìn t·∫°i/ƒë√∫ng t√™n?\n",
    "print(\"state col =\", CFG[\"state\"], \"| in df? ->\", CFG[\"state\"] in df.columns)\n",
    "print(\"ead   col =\", CFG[\"ead\"],   \"| in df? ->\", CFG[\"ead\"]   in df.columns)\n",
    "\n",
    "# 2) Ph√¢n b·ªë state hi·ªán t·∫°i\n",
    "print(df[CFG[\"state\"]].value_counts().head(10))\n",
    "\n",
    "# 3) T·ªïng EAD > 0 ?\n",
    "if CFG[\"ead\"] in df.columns:\n",
    "    print(\"Total EAD =\", df[CFG[\"ead\"]].fillna(0).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒë·∫£m b·∫£o c√≥ th·ª© t·ª± th·ªùi gian\n",
    "time_col = \"MOB\" if \"MOB\" in df.columns else CFG.get(\"cutoff\", None)\n",
    "assert time_col is not None, \"C·∫ßn MOB ho·∫∑c CUTOFF_DATE ƒë·ªÉ t·∫°o c·∫∑p chuy·ªÉn tr·∫°ng th√°i.\"\n",
    "\n",
    "# sort v√† t·∫°o state ·ªü t v√† t+1 theo t·ª´ng loan\n",
    "g = df.sort_values([CFG[\"loan\"], time_col]).groupby(CFG[\"loan\"], group_keys=False)\n",
    "\n",
    "pairs = []\n",
    "# state_t (hi·ªán t·∫°i), state_t1 (th√°ng sau), weight (EAD ho·∫∑c 1)\n",
    "tmp = pd.DataFrame({\n",
    "    \"state_t\":  g[CFG[\"state\"]].shift(0),\n",
    "    \"state_t1\": g[CFG[\"state\"]].shift(-1),\n",
    "    \"weight\":   (df[CFG[\"ead\"]] if CFG[\"ead\"] in df.columns else 1.0)\n",
    "})\n",
    "# c·∫ßn c√°c c·ªôt key ƒë·ªÉ gh√©p ƒë√∫ng\n",
    "tmp[CFG[\"loan\"]] = df[CFG[\"loan\"]].values\n",
    "tmp[time_col]    = df[time_col].values\n",
    "\n",
    "# ch·ªâ l·∫•y record c√≥ c·∫£ t v√† t+1 trong c√πng kho·∫£n vay\n",
    "tmp = tmp.dropna(subset=[\"state_t\",\"state_t1\"])\n",
    "print(\"pairs rows:\", len(tmp))\n",
    "tmp.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Loading data from Oracle...\n",
      "=== SQL DEBUG ===\n",
      "File: D:\\Python_code\\RR_Model\\sql\\NTB.sql\n",
      "First 200 chars:\n",
      " select  CUTOFF_DATE, AGREEMENT_ID, DISBURSAL_DATE, DISBURSAL_AMOUNT, DPD_EOM, RISK_BUCKET, PRINCIPLE_OUTSTANDING, STATUS, MOB, MAFC_SUB_CATEGORY PRODUCT_TYPE, NPA_STAGEID, DPD_GROUP, STATE_MODEL, CASE...\n",
      "Params: {}\n",
      "=================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python_code\\RR_Model\\src\\db.py:63: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(final_sql, conn, params=bind_params)\n"
     ]
    }
   ],
   "source": [
    "df = load_data(r\"D:\\Python_code\\RR_Model\\sql\\NTB.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m PARQUET_DIR \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# c√πng ƒë∆∞·ªùng d·∫´n m√† h√†m load d√πng\u001b[39;00m\n\u001b[0;32m      2\u001b[0m PARQUET_DIR\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPARQUET_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy_data.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\rrmodel\\lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\rrmodel\\lib\\site-packages\\pandas\\core\\frame.py:3124\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[1;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m   3043\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3044\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[0;32m   3045\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3120\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[0;32m   3121\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3122\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[1;32m-> 3124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_parquet(\n\u001b[0;32m   3125\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3126\u001b[0m     path,\n\u001b[0;32m   3127\u001b[0m     engine,\n\u001b[0;32m   3128\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3129\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   3130\u001b[0m     partition_cols\u001b[38;5;241m=\u001b[39mpartition_cols,\n\u001b[0;32m   3131\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3132\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3133\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\rrmodel\\lib\\site-packages\\pandas\\io\\parquet.py:478\u001b[0m, in \u001b[0;36mto_parquet\u001b[1;34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(partition_cols, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    477\u001b[0m     partition_cols \u001b[38;5;241m=\u001b[39m [partition_cols]\n\u001b[1;32m--> 478\u001b[0m impl \u001b[38;5;241m=\u001b[39m \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    480\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m    482\u001b[0m impl\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[0;32m    483\u001b[0m     df,\n\u001b[0;32m    484\u001b[0m     path_or_buf,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    491\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\rrmodel\\lib\\site-packages\\pandas\\io\\parquet.py:68\u001b[0m, in \u001b[0;36mget_engine\u001b[1;34m(engine)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m     66\u001b[0m             error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[1;32m---> 68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find a usable engine; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtried using: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA suitable version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to import the above resulted in these errors:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     76\u001b[0m     )\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[1;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "PARQUET_DIR = Path(\"data\")  # c√πng ƒë∆∞·ªùng d·∫´n m√† h√†m load d√πng\n",
    "PARQUET_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "df.to_parquet(PARQUET_DIR / \"my_data.parquet\", compression=\"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowKeyError",
     "evalue": "No type extension with name arrow.py_extension_type found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowKeyError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m PARQUET_DIR\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Ghi DataFrame ra file parquet b·∫±ng pyarrow\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPARQUET_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy_data.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# ƒê·ªçc l·∫°i file parquet\u001b[39;00m\n\u001b[0;32m     11\u001b[0m df_loaded \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(PARQUET_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_data.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\rrmodel\\lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\rrmodel\\lib\\site-packages\\pandas\\core\\frame.py:3124\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[1;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m   3043\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3044\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[0;32m   3045\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3120\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[0;32m   3121\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3122\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[1;32m-> 3124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_parquet(\n\u001b[0;32m   3125\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3126\u001b[0m     path,\n\u001b[0;32m   3127\u001b[0m     engine,\n\u001b[0;32m   3128\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3129\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   3130\u001b[0m     partition_cols\u001b[38;5;241m=\u001b[39mpartition_cols,\n\u001b[0;32m   3131\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3132\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3133\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\rrmodel\\lib\\site-packages\\pandas\\io\\parquet.py:478\u001b[0m, in \u001b[0;36mto_parquet\u001b[1;34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(partition_cols, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    477\u001b[0m     partition_cols \u001b[38;5;241m=\u001b[39m [partition_cols]\n\u001b[1;32m--> 478\u001b[0m impl \u001b[38;5;241m=\u001b[39m \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    480\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m    482\u001b[0m impl\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[0;32m    483\u001b[0m     df,\n\u001b[0;32m    484\u001b[0m     path_or_buf,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    491\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\rrmodel\\lib\\site-packages\\pandas\\io\\parquet.py:79\u001b[0m, in \u001b[0;36mget_engine\u001b[1;34m(engine)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find a usable engine; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtried using: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     76\u001b[0m     )\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPyArrowImpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FastParquetImpl()\n",
      "File \u001b[1;32m~\\.conda\\envs\\rrmodel\\lib\\site-packages\\pandas\\io\\parquet.py:170\u001b[0m, in \u001b[0;36mPyArrowImpl.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# import utils to register the pyarrow extension types\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension_types\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi \u001b[38;5;241m=\u001b[39m pyarrow\n",
      "File \u001b[1;32m~\\.conda\\envs\\rrmodel\\lib\\site-packages\\pandas\\core\\arrays\\arrow\\extension_types.py:174\u001b[0m\n\u001b[0;32m    167\u001b[0m     pyarrow\u001b[38;5;241m.\u001b[39mregister_extension_type(\n\u001b[0;32m    168\u001b[0m         ForbiddenExtensionType(pyarrow\u001b[38;5;241m.\u001b[39mnull(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marrow.py_extension_type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    169\u001b[0m     )\n\u001b[0;32m    171\u001b[0m     pyarrow\u001b[38;5;241m.\u001b[39m_hotfix_installed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m \u001b[43mpatch_pyarrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\rrmodel\\lib\\site-packages\\pandas\\core\\arrays\\arrow\\extension_types.py:166\u001b[0m, in \u001b[0;36mpatch_pyarrow\u001b[1;34m()\u001b[0m\n\u001b[0;32m    157\u001b[0m         pickletools\u001b[38;5;241m.\u001b[39mdis(serialized, out)\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    159\u001b[0m             _ERROR_MSG\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    160\u001b[0m                 storage_type\u001b[38;5;241m=\u001b[39mstorage_type,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m             )\n\u001b[0;32m    164\u001b[0m         )\n\u001b[1;32m--> 166\u001b[0m \u001b[43mpyarrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munregister_extension_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marrow.py_extension_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m pyarrow\u001b[38;5;241m.\u001b[39mregister_extension_type(\n\u001b[0;32m    168\u001b[0m     ForbiddenExtensionType(pyarrow\u001b[38;5;241m.\u001b[39mnull(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marrow.py_extension_type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    169\u001b[0m )\n\u001b[0;32m    171\u001b[0m pyarrow\u001b[38;5;241m.\u001b[39m_hotfix_installed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\rrmodel\\lib\\site-packages\\pyarrow\\types.pxi:2280\u001b[0m, in \u001b[0;36mpyarrow.lib.unregister_extension_type\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\rrmodel\\lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowKeyError\u001b[0m: No type extension with name arrow.py_extension_type found"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrowNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading pyarrow-22.0.0-cp310-cp310-win_amd64.whl.metadata (3.3 kB)\n",
      "Downloading pyarrow-22.0.0-cp310-cp310-win_amd64.whl (28.1 MB)\n",
      "   ---------------------------------------- 0.0/28.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/28.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.3/28.1 MB 4.8 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 2.4/28.1 MB 5.0 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 2.9/28.1 MB 4.7 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 4.7/28.1 MB 5.5 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 5.2/28.1 MB 5.1 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 6.8/28.1 MB 5.4 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 9.2/28.1 MB 6.1 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 11.0/28.1 MB 6.5 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 13.1/28.1 MB 7.0 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 14.9/28.1 MB 7.2 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 16.8/28.1 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 17.8/28.1 MB 7.2 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 18.9/28.1 MB 7.0 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 19.9/28.1 MB 6.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 21.0/28.1 MB 6.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 22.3/28.1 MB 6.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 23.3/28.1 MB 6.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 24.6/28.1 MB 6.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 25.7/28.1 MB 6.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 26.7/28.1 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  28.0/28.1 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 28.1/28.1 MB 6.4 MB/s  0:00:04\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-22.0.0\n"
     ]
    }
   ],
   "source": [
    "pip install pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CUTOFF_DATE  AGREEMENT_ID DISBURSAL_DATE  DISBURSAL_AMOUNT  DPD_EOM  \\\n",
      "0  2023-01-31       4089415     2023-01-31        18309000.0        0   \n",
      "1  2023-02-28       4089415     2023-01-31        18309000.0        0   \n",
      "2  2023-03-31       4089415     2023-01-31        18309000.0        0   \n",
      "3  2023-04-30       4089415     2023-01-31        18309000.0        0   \n",
      "4  2023-05-31       4089415     2023-01-31        18309000.0        0   \n",
      "\n",
      "  RISK_BUCKET  PRINCIPLE_OUTSTANDING STATUS  MOB PRODUCT_TYPE NPA_STAGEID  \\\n",
      "0          B0               18309000      A    0       SALPIL     REGULAR   \n",
      "1          B0               18273533      A    1       SALPIL     REGULAR   \n",
      "2          B0               17959922      A    2       SALPIL     REGULAR   \n",
      "3          B0               17703148      A    3       SALPIL     REGULAR   \n",
      "4          B0               17414031      A    4       SALPIL     REGULAR   \n",
      "\n",
      "  DPD_GROUP STATE_MODEL RISK_SCORE  \n",
      "0      DPD0        DPD0          B  \n",
      "1      DPD0        DPD0          B  \n",
      "2      DPD0        DPD0          B  \n",
      "3      DPD0        DPD0          B  \n",
      "4      DPD0        DPD0          B  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "PARQUET_DIR = Path(\"data\")\n",
    "PARQUET_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Ghi DataFrame ra file parquet b·∫±ng pyarrow\n",
    "df.to_parquet(PARQUET_DIR / \"my_data.parquet\", compression=\"gzip\", engine=\"pyarrow\", index=False)\n",
    "\n",
    "# ƒê·ªçc l·∫°i file parquet\n",
    "df_loaded = pd.read_parquet(PARQUET_DIR / \"my_data.parquet\", engine=\"pyarrow\")\n",
    "print(df_loaded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pandasNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Version: 2.3.3\n",
      "Summary: Powerful data structures for data analysis, time series, and statistics\n",
      "Home-page: https://pandas.pydata.org\n",
      "Author: \n",
      "Author-email: The Pandas Development Team <pandas-dev@python.org>\n",
      "License: BSD 3-Clause License\n",
      "         \n",
      "         Copyright (c) 2008-2011, AQR Capital Management, LLC, Lambda Foundry, Inc. and PyData Development Team\n",
      "         All rights reserved.\n",
      "         \n",
      "         Copyright (c) 2011-2023, Open source contributors.\n",
      "         \n",
      "         Redistribution and use in source and binary forms, with or without\n",
      "         modification, are permitted provided that the following conditions are met:\n",
      "         \n",
      "         * Redistributions of source code must retain the above copyright notice, this\n",
      "           list of conditions and the following disclaimer.\n",
      "         \n",
      "         * Redistributions in binary form must reproduce the above copyright notice,\n",
      "           this list of conditions and the following disclaimer in the documentation\n",
      "           and/or other materials provided with the distribution.\n",
      "         \n",
      "         * Neither the name of the copyright holder nor the names of its\n",
      "           contributors may be used to endorse or promote products derived from\n",
      "           this software without specific prior written permission.\n",
      "         \n",
      "         THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
      "         AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
      "         IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
      "         DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
      "         FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
      "         DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
      "         SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
      "         CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
      "         OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
      "         OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
      "         \n",
      "Location: c:\\users\\mafc4709\\.conda\\envs\\rrmodel\\lib\\site-packages\n",
      "Requires: numpy, python-dateutil, pytz, tzdata\n",
      "Required-by: seaborn\n",
      "---\n",
      "Name: pyarrow\n",
      "Version: 22.0.0\n",
      "Summary: Python library for Apache Arrow\n",
      "Home-page: https://arrow.apache.org/\n",
      "Author: \n",
      "Author-email: \n",
      "License: Apache Software License\n",
      "Location: c:\\users\\mafc4709\\.conda\\envs\\rrmodel\\lib\\site-packages\n",
      "Requires: \n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "pip show pandas pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Gi·∫£ s·ª≠ DataFrame c·ªßa b·∫°n t√™n l√† df\n",
    "df.to_parquet(\"output.parquet\", compression=\"snappy\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Loading data from Oracle...\n",
      "=== SQL DEBUG ===\n",
      "File: D:\\Python_code\\RR_Model\\sql\\ETB.sql\n",
      "First 200 chars:\n",
      " select  CUTOFF_DATE, AGREEMENT_ID, DISBURSAL_DATE, DISBURSAL_AMOUNT, DPD_EOM, RISK_BUCKET, PRINCIPLE_OUTSTANDING, STATUS, MOB, MAFC_SUB_CATEGORY PRODUCT_TYPE, NPA_STAGEID, DPD_GROUP, STATE_MODEL, CASE...\n",
      "Params: {}\n",
      "=================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python_code\\RR_Model\\src\\db.py:63: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(final_sql, conn, params=bind_params)\n"
     ]
    }
   ],
   "source": [
    "df = load_data(r\"D:\\Python_code\\RR_Model\\sql\\ETB.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Gi·∫£ s·ª≠ DataFrame c·ªßa b·∫°n t√™n l√† df\n",
    "df.to_parquet(\"ETB.parquet\", compression=\"snappy\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
